{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import org.apache.spark.sql.SQLContext\n",
    "\n",
    "val sqlContext = new SQLContext(sc)\n",
    "\n",
    "val df = sqlContext.read.json(\"data/people/people.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import sqlContext.implicits._\n",
    " \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "df.select($\"name\",$\"age\"+1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n",
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|null|    1|\n",
      "|  19|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "// Select people older than 21\n",
    "df.filter($\"age\">21).show()\n",
    "\n",
    "// Count people by age\n",
    "df.groupBy(\"age\").count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "df.registerTempTable(\"df\")\n",
    "sqlContext.sql(\"SELECT * from df\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Justin\n",
      "Name: Justin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import sqlContext.implicits._\n",
    "\n",
    "case class Person(name: String, age: Int)\n",
    "\n",
    "// Infer the schema, and register the DataFrame as a table.\n",
    "\n",
    " val schemaPeople= sqlContext.sparkContext.textFile(\"data/people/people.txt\")\n",
    "         .map(l => l.split(\",\") )\n",
    "         .map(p => Person(p(0), p(1).trim.toInt))\n",
    "         .toDF()\n",
    "schemaPeople.registerTempTable(\"people\")\n",
    "\n",
    "// SQL can be run over DataFrames that have been registered as a table. Complete the following query\n",
    "// to return teenagers, i.e., age >= 13 and age <= 19.\n",
    "val teenagers = sqlContext.sql(\"SELECT name FROM people WHERE age BETWEEN 13 AND 19\")\n",
    "\n",
    "// The results of SQL queries are DataFrames and support all the normal RDD operations.\n",
    "// The columns of a row in the result can be accessed by field index:\n",
    "teenagers.map(t => \"Name: \" + t(0)).collect().foreach(println)\n",
    "\n",
    "// or by field name:\n",
    "teenagers.map(t => \"Name: \" + t.getAs[String](\"name\")).collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Michael\n",
      "Name: Andy\n",
      "Name: Justin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "val people = sc.textFile(\"data/people/people.txt\")\n",
    "\n",
    "// The schema is encoded in a string\n",
    "val schemaString = \"name age\"\n",
    "\n",
    "// Import Row.\n",
    "import org.apache.spark.sql.Row;\n",
    "\n",
    "// Import Spark SQL data types\n",
    "import org.apache.spark.sql.types.{StructType,StructField,StringType};\n",
    "\n",
    "// Generate the schema based on the string of schema\n",
    "val schema =\n",
    "  StructType(\n",
    "    schemaString.split(\" \").map(fieldName => StructField(fieldName, StringType, true)))\n",
    "\n",
    "// Convert records of the RDD (people) to Rows.\n",
    "val rowRDD = people.map(_.split(\",\")).map(p => Row(p(0), p(1).trim))\n",
    "\n",
    "// Apply the schema to the RDD.\n",
    "val peopleDataFrame = sqlContext.createDataFrame(rowRDD, schema)\n",
    "\n",
    "// Register the DataFrames as a table.\n",
    "peopleDataFrame.registerTempTable(\"people\")\n",
    "\n",
    "// SQL statements can be run by using the sql methods provided by sqlContext.\n",
    "val results = sqlContext.sql(\"SELECT name FROM people\")\n",
    "\n",
    "// The results of SQL queries are DataFrames and support all the normal RDD operations.\n",
    "// The columns of a row in the result can be accessed by field index or by field name.\n",
    "results.map(t => \"Name: \" + t(0)).collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just run this code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "// Just run this code\n",
    "// Load data from a parquet file\n",
    "val df = sqlContext.read.load(\"data/people/people.parquet\")\n",
    "df.select(\"name\", \"favorite_color\").write.mode(\"overwrite\").save(\"namesAndFavColors.parquet\")\n",
    "\n",
    "// Manually specify the data source type, e.g., json, parquet, jdbc.\n",
    "val jdf = sqlContext.read.format(\"json\").load(\"data/people/people.json\")\n",
    "jdf.select(\"name\", \"age\").write.format(\"parquet\").mode(\"overwrite\").save(\"namesAndAges.parquet\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Justin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "// Just run this code\n",
    "// The RDD is implicitly converted to a DataFrame by implicits, allowing it to be stored using Parquet.\n",
    "schemaPeople.write.parquet(\"people.parquet\")\n",
    "\n",
    "// Read in the parquet file created above.  Parquet files are self-describing so the schema is preserved.\n",
    "// The result of loading a Parquet file is also a DataFrame.\n",
    "val parquetFile = sqlContext.read.parquet(\"people.parquet\")\n",
    "\n",
    "//Parquet files can also be registered as tables and then used in SQL statements.\n",
    "parquetFile.registerTempTable(\"parquetFile\")\n",
    "val teenagers = sqlContext.sql(\"SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\")\n",
    "teenagers.map(t => \"Name: \" + t(0)).collect().foreach(println)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "[address: struct<city:string,state:string>, name: string]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Just run this code\n",
    "// A JSON dataset is pointed to by path.\n",
    "// The path can be either a single text file or a directory storing text files.\n",
    "val people = sqlContext.read.json(\"data/people/people.json\")\n",
    "\n",
    "// The inferred schema can be visualized using the printSchema() method.\n",
    "people.printSchema()\n",
    "// root\n",
    "//  |-- age: integer (nullable = true)\n",
    "//  |-- name: string (nullable = true)\n",
    "\n",
    "// Register this DataFrame as a table.\n",
    "people.registerTempTable(\"people\")\n",
    "\n",
    "// SQL statements can be run by using the sql methods provided by sqlContext.\n",
    "val teenagers = sqlContext.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "// Alternatively, a DataFrame can be created for a JSON dataset represented by\n",
    "// an RDD[String] storing one JSON object per string.\n",
    "val anotherPeopleRDD = sc.parallelize(\n",
    "  \"\"\"{\"name\":\"Yin\",\"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"}}\"\"\" :: Nil)\n",
    "val anotherPeople = sqlContext.read.json(anotherPeopleRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
